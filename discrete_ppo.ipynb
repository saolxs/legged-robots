{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    # fmt: off\n",
    "    file = os.getcwd()\n",
    "    parser = argparse.ArgumentParser()\n",
    "   # parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(file%.*).rstrip(\".ipynb\"), (basename ${file%.*})\n",
    "        #help=\"the name of this experiment\")\n",
    "    parser.add_argument(\"--gym-id\", type=str, default=\"CartPole-v1\",\n",
    "        help=\"the id of the gym environment\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=1e-4,\n",
    "        help=\"the learning rate of the optimizer\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42,\n",
    "        help=\"seed of the experiment\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=25000,\n",
    "        help=\"total timesteps of the experiments\")\n",
    "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
    "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, cuda will be enabled by default\")\n",
    "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
    "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"ppo-implementation-details\",\n",
    "        help=\"the wandb's project name\")\n",
    "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
    "        help=\"the entity (team) of wandb's project\")\n",
    "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    parser.add_argument(\"--num-envs\", type=int, default=4,\n",
    "        help=\"the number of parallel game environments\")\n",
    "    parser.add_argument(\"--num-steps\", type=int, default=128,\n",
    "        help=\"the number of steps to run in each environment per policy rollout\")\n",
    "    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggle learning rate annealing for policy and value networks\")\n",
    "    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Use GAE for advantage computation\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "        help=\"the discount factor gamma\")\n",
    "    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n",
    "        help=\"the lambda for the general advantage estimation\")\n",
    "    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n",
    "        help=\"the number of mini-batches\")\n",
    "    parser.add_argument(\"--update-epochs\", type=int, default=4,\n",
    "        help=\"the K epochs to update the policy\")\n",
    "    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggles advantages normalization\")\n",
    "    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n",
    "        help=\"the surrogate clipping coefficient\")\n",
    "    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n",
    "    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n",
    "        help=\"coefficient of the entropy\")\n",
    "    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n",
    "        help=\"coefficient of the value function\")\n",
    "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n",
    "        help=\"the maximum norm for the gradient clipping\")\n",
    "    parser.add_argument(\"--target-kl\", type=float, default=None,\n",
    "        help=\"the target KL divergence threshold\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "    # fmt: on\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(gym_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(gym_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, args):\n",
    "        device = args.device\n",
    "        envs = args.gym_id\n",
    "        \n",
    "        # Storage setup\n",
    "        self.state = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "        self.actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "        self.logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "        self.values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "        self.rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "        self.dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "        \n",
    "    def get_batches(self):\n",
    "        # Optimizing the policy and value network\n",
    "        indices = np.arange(args.batch_size)\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            batch_inds = [indices[i:i + args.minibatch_size] for i in range(0, args.batch_size, args.minibatch_size)]\n",
    "                \n",
    "            return batch_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layer(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    #if isinstance(layer, nn.Linear):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer \n",
    "    \n",
    "class Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, envs, chkpt_dir='tmp/ppo'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.checkpoint = os.path.join(chkpt_dir, 'actor_critic_ppo')\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            init_layer(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            init_layer(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            init_layer(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def action_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "            \n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint)\n",
    "    \n",
    "    def load(self):\n",
    "        torch.load_state_dict(torch.load(self.checkpoint))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    args = parse_args()\n",
    "    run_name = f\"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    #seeds\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    \n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.gym_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "    \n",
    "    agent = Agent(envs).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "    print(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/olvdejo/Desktop/Research Project/code/cartpole'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalized Advantage Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(envs, nxt_state, nxt_value, rewards, args): \n",
    "        \n",
    "        if args.gae:\n",
    "            advantages = torch.zeros_like(rewards).to(device)\n",
    "            gae = 0\n",
    "            returns = []\n",
    "            for step in reversed(range(args.num_steps)):\n",
    "                if step == args.num_steps - 1:\n",
    "                    mask_terminal  = 1.0 - nxt_done\n",
    "                    nextvalues = nxt_value\n",
    "                else:\n",
    "                    mask_terminal = 1.0 - dones[step + 1]\n",
    "                    nextvalues = values[step + 1]\n",
    "                    \n",
    "                delta = rewards[step] + args.gamma * nextvalues * mask_terminal - values[step]\n",
    "                \n",
    "                advantages[step] = gae = delta + args.gamma * args.gae_lambda * mask_terminal * gae\n",
    "                \n",
    "            returns.insert(0, advantages + values)\n",
    "            \n",
    "        else:\n",
    "            returns = torch.zeros_like(rewards).to(device)\n",
    "            advantages = []\n",
    "            for step in reversed(range(args.num_steps)):\n",
    "                if step == args.num_steps - 1:\n",
    "                    mask_terminal= 1.0 - nxt_done\n",
    "                    nxt_return = nxt_value\n",
    "                else:\n",
    "                    mask_terminal = 1.0 - dones[step + 1]\n",
    "                    nxt_return = returns[step + 1]\n",
    "                returns[step] = rewards[step] + args.gamma * mask_terminal * nxt_return\n",
    "            advantages.insert(0, returns - values)\n",
    "            \n",
    "        return advantages, returns      \n",
    "\n",
    "def normalize(x):\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-8)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(states, log_probs, actions, returns, advantages):\n",
    "    \n",
    "    \n",
    "    \n",
    "    advantages, returns = compute_gae(envs, nxt_state, rewards, args)\n",
    "\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    nxt_state = torch.Tensor(args.envs.reset()).to(device)\n",
    "    nxt_done = torch.zeros(args.num_envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "    \n",
    "\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Annealing the rate if instructed to do so.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += 1 * args.num_envs\n",
    "            state[step] = nxt_state\n",
    "            dones[step] = nxt_done\n",
    "\n",
    "            # ALGO LOGIC: action logic\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.action_value(nxt_state)\n",
    "                values[step] = value.flatten()\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # TRY NOT TO MODIFY: execute the game and log data.\n",
    "            nxt_state, reward, done, info = envs.step(action.cpu().numpy())\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            nxt_state, nxt_done = torch.Tensor(nxt_state).to(device), torch.Tensor(done).to(device)\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            for item in info:\n",
    "                if \"episode\" in item.keys():\n",
    "                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                    break\n",
    "            '''\n",
    "                \n",
    "    yield state.reshape((-1,) + envs.single_observation_space.shape), logprobs.reshape(-1), actions.reshape((-1,) + envs.single_action_space.shape), advantages.reshape(-1), returns.reshape(-1), values.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "def update_ppo(envs, states, log_probs, actions, returns, advantages):\n",
    "    \n",
    "    # Storage setup\n",
    "    # flatten the batch\n",
    "    \n",
    "    \n",
    "    for batch_states, batch_logprobs, batch_actions, batch_advantages, batch_returns, batch_values in get_batches(states, log_probs, actions, returns, advantages):\n",
    "        \n",
    "    \n",
    "                \n",
    "                \n",
    "    # flatten the batch\n",
    "    advantages, returns = compute_gae(envs, nxt_state, rewards, args)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    batch_inds = np.arange(args.batch_size)\n",
    "    clip_fracs = []\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for start in range(0, args.batch_size, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = batch_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_value(batch_obs[mb_inds], batch_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - batch_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clip_fracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = batch_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - batch_returns[mb_inds]) ** 2\n",
    "                v_clipped = batch_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - batch_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - batch_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - batch_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.target_kl is not None:\n",
    "            if approx_kl > args.target_kl:\n",
    "                break\n",
    "            \n",
    "    y_pred, y_true = batch_values.cpu().numpy(), batch_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ppo(envs, nxt_state, rewards, args):\n",
    "    advantages, returns = compute_gae(envs, nxt_state, rewards, args)\n",
    "     \n",
    "    # flatten the batch\n",
    "    batch_obs = state.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    batch_logprobs = logprobs.reshape(-1)\n",
    "    batch_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    batch_advantages = advantages.reshape(-1)\n",
    "    batch_returns = returns.reshape(-1)\n",
    "    batch_values = values.reshape(-1)\n",
    "    \n",
    "    #policy and value network optimisation\n",
    "    batch_idx = np.arange(args.batch_size)\n",
    "    for epoch in range(args.update_epochs):\n",
    "        \n",
    "        np.random.shuffle(batch_idx)\n",
    "        \n",
    "        for i in range(0, args.batch_size, args.minibatch_size):\n",
    "                clipfracs = []\n",
    "                batch_inds = mem.get_batches()\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_value(b_obs[batch_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[batch_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[batch_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "        if args.target_kl is not None:\n",
    "            if approx_kl > args.target_kl:\n",
    "                break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        \n",
    "    args = parse_args()\n",
    "    run_name = f\"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    #seeds\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    # env setup\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.gym_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "    device = args.device\n",
    "    agent = Agent(envs).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "    state = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        agent = Agent(envs).to(device)\n",
    "        \n",
    "        \n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        \n",
    "        nxt_value = agent.value(nxt_state).reshape(1, -1)\n",
    "        \n",
    "        \n",
    "        advantage_hist, return_hist = compute_gae(envs, nxt_state, rewards, args)\n",
    "\n",
    "        \n",
    "\n",
    "        for epoch in range(args.update_epochs):\n",
    "            \n",
    "            for i in range(0, args.batch_size, args.minibatch_size):\n",
    "                    clipfracs = []\n",
    "                    batch_inds = mem.get_batches()\n",
    "\n",
    "                    _, newlogprob, entropy, newvalue = agent.get_action_value(b_obs[batch_inds], b_actions.long()[mb_inds])\n",
    "                    logratio = newlogprob - b_logprobs[batch_inds]\n",
    "                    ratio = logratio.exp()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                        old_approx_kl = (-logratio).mean()\n",
    "                        approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                        clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                    mb_advantages = b_advantages[batch_inds]\n",
    "                    if args.norm_adv:\n",
    "                        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                    # Policy loss\n",
    "                    pg_loss1 = -mb_advantages * ratio\n",
    "                    pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                    pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                    # Value loss\n",
    "                    newvalue = newvalue.view(-1)\n",
    "                    if args.clip_vloss:\n",
    "                        v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                        v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                            newvalue - b_values[mb_inds],\n",
    "                            -args.clip_coef,\n",
    "                            args.clip_coef,\n",
    "                        )\n",
    "                        v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                        v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                        v_loss = 0.5 * v_loss_max.mean()\n",
    "                    else:\n",
    "                        v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                    entropy_loss = entropy.mean()\n",
    "                    loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                    optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None:\n",
    "                if approx_kl > args.target_kl:\n",
    "                    break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e464f6065abbcf73e173654187ddf7a6c58253bc8120564044a1282e6289ab0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
